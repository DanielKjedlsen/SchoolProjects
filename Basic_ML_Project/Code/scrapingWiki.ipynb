{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from os.path import exists\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from hf import clean_text, replaceWithNull, standard_clean, hard_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The specified url and the letters for our group.\n",
    "url = \"https://en.wikinews.org/wiki/Category:Politics_and_conflicts\"\n",
    "signs = [\"H\",\"I\",\"J\",\"K\",\"L\",\"M\",\"N\",\"O\",\"P\",\"R\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_exists = exists(\"../Data/wiki.csv\")\n",
    "if file_exists == False: \n",
    "    # We use the libaries request and beautifulsoup.\n",
    "    website = requests.get(url)\n",
    "    soup = BeautifulSoup(website.content, 'html.parser')\n",
    "    data = soup.findAll(\"div\", {\"class\":\"mw-category-group\"})\n",
    "\n",
    "    # Extracting all subcategories of articles that contain our letters.\n",
    "    _data = []\n",
    "    for element in data:\n",
    "        if element.find(\"h3\").contents[0] in signs:\n",
    "            for href in element.findAll('a', href=True):\n",
    "                _data.append(href['href'])\n",
    "\n",
    "    # We now use the links to find the links in subpages.\n",
    "    _data2 = []\n",
    "    for element in _data:\n",
    "        response = requests.get('https://en.wikinews.org/' + element)\n",
    "        text_in_response = response.text\n",
    "        soup_a = BeautifulSoup(text_in_response, 'html.parser')\n",
    "        temp = soup_a.findAll('div', id='mw-pages')\n",
    "        \n",
    "        for element in temp:\n",
    "            temp2 = element.findAll('a')\n",
    "            for element in temp2:\n",
    "                _data2.append(element['href'])\n",
    "\n",
    "                dates = []\n",
    "    titles = []\n",
    "    content = []\n",
    "    \n",
    "    for element1 in tqdm(_data2):\n",
    "        response = requests.get('https://en.wikinews.org/' + element1)\n",
    "        soup_b = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        #Extract date from text. Some cases does not have a date, if so we insert N/A\n",
    "        try: # look for HTML element\n",
    "            dates.append(soup_b.find('strong', class_=\"published\").contents[1])\n",
    "        except: # add NA if nothing found\n",
    "            dates.append(\"N/A\")\n",
    "        \n",
    "        #Extract title of article\n",
    "        try:\n",
    "            titles.append(soup_b.find('h1', class_=\"firstHeading mw-first-heading\").contents[0])\n",
    "        #titles.append(title)\n",
    "        except:  \n",
    "            dates.append(\"N/A\")\n",
    "            \n",
    "        #Extract main content from the article\n",
    "        article_content = \"\"\n",
    "        for sub_text in soup_b.find_all('p'):\n",
    "            sub_content = str(sub_text.text)\n",
    "            article_content += sub_content\n",
    "        \n",
    "        content.append(article_content)\n",
    "        dfw = pd.DataFrame(list(zip(titles, dates, content)), columns =['Title', 'Date published', 'Article content'])\n",
    "         \n",
    "else: \n",
    "    file = Path(\"../Data/wiki.csv\")\n",
    "    dfw = pd.read_csv(file)\n",
    "    #dfw = pd.DataFrame(list(zip(titles, dates, content)), columns =['Title', 'Date published', 'Article content'])\n",
    "    dfw.to_csv(file, index=False)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 3)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48b8a3cf9bc34046b01f0ac3942ccf53",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06f802ee103b4dce9115315ea0540200",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/212 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "file = Path(\"../Data/wiki.csv\")\n",
    "print (dfw.shape)\n",
    "dfw.to_csv(file, index=False)\n",
    "\n",
    "# Code for reading the csv file and separating.\n",
    "data = []\n",
    "\n",
    "with open (file, 'r', encoding='utf-8') as reader: \n",
    "    reader = pd.read_csv(reader, chunksize=1000,quotechar='\"', delimiter=',')\n",
    "    for chunk in (reader): \n",
    "        chunk = replaceWithNull(chunk)\n",
    "        for row_no in tqdm(range(len(chunk.index)), leave=False , position = 0):\n",
    "            for col_no in range (len(chunk.columns)):\n",
    "                chunk.iloc[row_no,col_no] = clean_text((chunk.iloc[row_no,col_no]))\n",
    "        data.append(chunk)\n",
    "\n",
    "reader.close()\n",
    "data = pd.concat(data)\n",
    "# Converting to a pandas dataframe. \n",
    "dfw = pd.DataFrame(data)   \n",
    "\n",
    "dfw.columns = ['Title', 'Date published', 'Article content']\n",
    "\n",
    "dfw.to_csv(file, index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "901b79e026e03396fd1ffa7133844e9ea80e258ce34c66e1aabb5896bcb18463"
  },
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
