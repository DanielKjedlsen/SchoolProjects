{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the file that orders the scraped data from the wiki and inserts it into SQL "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and such"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import trange, tqdm\n",
    "from hf import psyCop, clean_text, replaceWithNull, clean_textLigt\n",
    "import numpy as np\n",
    "from os.path import exists\n",
    "import time\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "#from tqdm.notebook import trange, tqdm\n",
    "from hf import  clean_text, replaceWithNull\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTid = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"../Data/wiki.csv\")\n",
    "\n",
    "# Code for reading the csv file and separating.\n",
    "data = []\n",
    "i = 0\n",
    "breakpoint = 100\n",
    "chunksize = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (file, 'r', encoding='utf-8') as reader: \n",
    "    reader = pd.read_csv(reader, chunksize=chunksize,quotechar='\"', delimiter=',')\n",
    "    for chunk in reader:\n",
    "        replaceWithNull(chunk)\n",
    "        for column in chunk.columns:\n",
    "            chunk[column] = chunk[column].astype(str)\n",
    "            chunk[column] = chunk[column].str.replace(\"\\n\", \"\")\n",
    "            chunk[column] = chunk[column].replace(\"nan\", \"null\")\n",
    "        data.append(chunk)\n",
    "        i += 1\n",
    "        if i >= breakpoint:\n",
    "            break           \n",
    "reader.close()\n",
    "data = pd.concat(data)\n",
    "\n",
    "# Converting to a pandas dataframe. \n",
    "df = pd.DataFrame(data)   \n",
    "df.columns = ['Title' ,'Date published','Article contentNone']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1212, 3)\n",
      "0          sunday   DATE   NUM  \n",
      "1        saturday   DATE   NUM  \n",
      "2          monday   DATE   NUM  \n",
      "3          monday   DATE   NUM  \n",
      "4       wednesday   DATE   NUM  \n",
      "                  ...           \n",
      "1207       friday   DATE   NUM  \n",
      "1208       friday   DATE   NUM  \n",
      "1209      tuesday   DATE   NUM  \n",
      "1210      tuesday   DATE   NUM  \n",
      "1211      tuesday   DATE   NUM  \n",
      "Name: Date published, Length: 1212, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print (df.shape)\n",
    "print (df.iloc[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droper Tables hvis de eksitere \n",
    "drop_tables = (\"\"\" DROP TABLE IF EXISTS pap_article\"\"\") \n",
    "\n",
    "#Tables til Wikipedia\n",
    "create_politics_table = (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS pap_article(\n",
    "    id  SERIAL PRIMARY KEY,\n",
    "    title VARCHAR,\n",
    "    published VARCHAR,\n",
    "    content VARCHAR     \n",
    "    )\n",
    "    \"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = None\n",
    "try:\n",
    "    conn = psycopg2.connect(#database=\"public\",\n",
    "                        port = \"5432\",\n",
    "                        host=\"localhost\", \n",
    "                        user=\"postgres\", \n",
    "                        password=\"1234\")\n",
    "except psycopg2.DatabaseError as e: \n",
    "        print (f'Error {e}')\n",
    "        sys.exit(1)\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "cur.execute(drop_tables)\n",
    "cur.execute(create_politics_table)\n",
    "\n",
    "with open('../Data/wiki.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"pap_article\", columns=('title', \"published\", \"content\"), sep=\",\", null=\"\")\n",
    "\n",
    "# Make the changes to the database persistent\n",
    "conn.commit()\n",
    "\n",
    "# Close communication with the database\n",
    "cur.close()\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "425c1cb9f96b02dd326489ac6b96e3042bda6ec4abf5f4b3f4fc167f5a6a6fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
