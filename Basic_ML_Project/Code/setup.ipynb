{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the setup for the final project. The idea is to run this file once or as few times as possible. \n",
    "The only veriable you need to change are the breakpoint and chunksize, since the control how many rows the project will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Since the GPL-licensed package `unidecode` is not installed, using Python's `unicodedata` package which yields worse results.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "import pandas as pd\n",
    "from pandas.core.common import SettingWithCopyWarning\n",
    "warnings.simplefilter(action=\"ignore\", category=SettingWithCopyWarning)\n",
    "import psycopg2\n",
    "from pathlib import Path\n",
    "from hf import replaceWithNull, standard_clean, hard_clean , time_clean, create_relationship, write_to_csv\n",
    "import numpy as np\n",
    "from cleantext import clean\n",
    "from os.path import exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "startTid = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data to Python to CSV.\n",
    "In the codeblock below we are opening the large CSV file and running 100 round of 10.000 rows each for a total of 1mil rows. \n",
    "The result are then put into a pandas dataframe. The runtime is about 30min. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = Path(\"../Data/1mio-raw.csv\")\n",
    "\n",
    "# Code for reading the csv file and separating.\n",
    "data = []\n",
    "i = 0\n",
    "breakpoint = 1000\n",
    "chunksize = 1000\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open (file, 'r', encoding='utf-8') as reader: \n",
    "    reader = pd.read_csv(reader, chunksize=chunksize,quotechar='\"', delimiter=',')\n",
    "    for chunk in reader:\n",
    "        replaceWithNull(chunk)\n",
    "        for column in chunk.columns:\n",
    "            chunk[column] = chunk[column].astype(str)\n",
    "            chunk[column] = chunk[column].str.replace(\"\\n\", \"\")\n",
    "            chunk[column] = chunk[column].replace(\"nan\", \"null\")\n",
    "        data.append(chunk)\n",
    "        i += 1\n",
    "        if i >= breakpoint:\n",
    "            break           \n",
    "reader.close()\n",
    "data = pd.concat(data)\n",
    "\n",
    "# Converting to a pandas dataframe. \n",
    "df = pd.DataFrame(data)   \n",
    "df.columns = ['None', 'Id', 'Domain', 'Type', 'Url', 'Content', \n",
    "                                   'Scraped_at', 'Inserted_at', 'Updated_at', 'Title', \n",
    "                                   'Authors', 'Keywords', 'Meta_keywords', \n",
    "                                   'Meta_description', 'Tags', 'Summary', 'Source']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup of SQL \n",
    "\n",
    "This is our two veriable where we store our SQL commands. First of we drop all of the  tables and we then create them. We used normalization and wrote our database in BCNF to avoid redundancy and exponentially large tables. We left a lot of functional dependencys in the articles table but chose not to divide it into smaller tables because we prioritised simplification of queries \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Belows we are setting up a connection to our local SQL server and first droping the tables (if any) and then creating the tables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Droper Tables hvis de eksitere \n",
    "drop_tables = (\"\"\" DROP TABLE IF EXISTS article, url, author, keyword, meta_keyword, tag, article_tag, article_author, article_keyword, article_meta_keyword, Wiki\"\"\") \n",
    "\n",
    "#Samling af instruktioner til at lave flere tables\n",
    "create_fake_news_corpus_tables =  (\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS article(\n",
    "    id VARCHAR(256) PRIMARY KEY,\n",
    "    type VARCHAR(50),\n",
    "    content VARCHAR,  \n",
    "    scraped_at DATE,\n",
    "    inserted_at DATE,\n",
    "    updated_at DATE,\n",
    "    title VARCHAR(250),\n",
    "    meta_description VARCHAR,\n",
    "    summary VARCHAR(100),\n",
    "    source VARCHAR(50)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS url(\n",
    "    id VARCHAR(50),\n",
    "    article_id VARCHAR(50),\n",
    "    sld VARCHAR(50),\n",
    "    tld VARCHAR(50),\n",
    "    slug VARCHAR,\n",
    "    ssl VARCHAR(3),\n",
    "    PRIMARY KEY(id, article_id),\n",
    "    FOREIGN KEY (article_id) REFERENCES article(id)\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS author(\n",
    "    id VARCHAR(50) PRIMARY KEY, \n",
    "    name VARCHAR\n",
    "    );\n",
    "    \n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS article_author(\n",
    "    article_id VARCHAR(50),\n",
    "    author_id VARCHAR(50),\n",
    "    PRIMARY KEY (article_id, author_id),\n",
    "    FOREIGN KEY (article_id) REFERENCES article(id),\n",
    "    FOREIGN KEY (author_id) REFERENCES author(id)\n",
    "    );\n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS keyword(\n",
    "    id VARCHAR(50) PRIMARY KEY, \n",
    "    label VARCHAR(50)\n",
    "    );\n",
    "\n",
    "       \n",
    "    CREATE TABLE IF NOT EXISTS article_keyword(\n",
    "    article_id VARCHAR(50),\n",
    "    keyword_id VARCHAR(50),\n",
    "    PRIMARY KEY (article_id, keyword_id),\n",
    "    FOREIGN KEY (article_id) REFERENCES article(id),\n",
    "    FOREIGN KEY (keyword_id) REFERENCES keyword(id)\n",
    "    );\n",
    "    \n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS meta_keyword(\n",
    "    id VARCHAR(50) PRIMARY KEY, \n",
    "    label VARCHAR\n",
    "    );\n",
    "\n",
    "    CREATE TABLE IF NOT EXISTS article_meta_keyword( \n",
    "    article_id VARCHAR(50),\n",
    "    meta_keyword_id VARCHAR(50),\n",
    "    PRIMARY KEY (article_id, meta_keyword_id),\n",
    "    FOREIGN KEY (article_id) REFERENCES article(id),\n",
    "    FOREIGN KEY (meta_keyword_id) REFERENCES meta_keyword(id)\n",
    "    );\n",
    "    \n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS tag(\n",
    "    id VARCHAR(50) PRIMARY KEY, \n",
    "    label VARCHAR\n",
    "    );\n",
    "    \n",
    "    \n",
    "    CREATE TABLE IF NOT EXISTS article_tag( \n",
    "    article_id VARCHAR(50),\n",
    "    tag_id VARCHAR(50),\n",
    "    PRIMARY KEY (article_id, tag_id),\n",
    "    FOREIGN KEY (article_id) REFERENCES article(id),\n",
    "    FOREIGN KEY (tag_id) REFERENCES tag(id)\n",
    "    );\n",
    "    \"\"\"\n",
    ")\n",
    "\n",
    "alter_tables = (\"\"\"\n",
    "    ALTER TABLE article\n",
    "    ALTER COLUMN type\n",
    "    TYPE VARCHAR(50)\n",
    "    USING\n",
    "        CASE type\n",
    "            WHEN 'null' THEN NULL\n",
    "            WHEN '' THEN NULL\n",
    "            ELSE type::VARCHAR(50)\n",
    "        END;\n",
    "\n",
    "    ALTER TABLE article\n",
    "    ALTER COLUMN summary\n",
    "    TYPE VARCHAR(100)\n",
    "    USING\n",
    "        CASE summary\n",
    "            WHEN 'null' THEN NULL\n",
    "            WHEN '' THEN NULL\n",
    "            ELSE summary::VARCHAR(100)\n",
    "        END;\n",
    "\n",
    "    ALTER TABLE article\n",
    "    ALTER COLUMN source\n",
    "    TYPE VARCHAR(50)\n",
    "    USING\n",
    "        CASE source\n",
    "            WHEN 'null' THEN NULL\n",
    "            WHEN '' THEN NULL\n",
    "            ELSE source::VARCHAR(50)\n",
    "        END;\n",
    "\n",
    "    ALTER TABLE author\n",
    "    ALTER COLUMN name\n",
    "    TYPE VARCHAR\n",
    "    USING\n",
    "        CASE name\n",
    "            WHEN 'null' THEN NULL\n",
    "            WHEN '' THEN NULL\n",
    "            ELSE name::VARCHAR\n",
    "        END;\n",
    "\n",
    "    ALTER TABLE keyword\n",
    "    ALTER COLUMN label\n",
    "    TYPE VARCHAR\n",
    "    USING\n",
    "        CASE label\n",
    "            WHEN 'null' THEN NULL\n",
    "            WHEN '' THEN NULL\n",
    "            ELSE label::VARCHAR\n",
    "        END;\n",
    "\n",
    "    ALTER TABLE meta_keyword\n",
    "    ALTER COLUMN label\n",
    "    TYPE VARCHAR\n",
    "    USING\n",
    "        CASE label\n",
    "            WHEN 'null' THEN NULL\n",
    "            WHEN '' THEN NULL\n",
    "            ELSE label::VARCHAR\n",
    "        END;\n",
    "\n",
    "    ALTER TABLE tag\n",
    "    ALTER COLUMN label\n",
    "    TYPE VARCHAR\n",
    "    USING\n",
    "        CASE label\n",
    "            WHEN 'null' THEN NULL\n",
    "            WHEN '' THEN NULL\n",
    "            ELSE label::VARCHAR\n",
    "        END;\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning data\n",
    "df[\"Authors\"] = df[\"Authors\"].str.replace(\"America, There\", \"America There\")\n",
    "\n",
    "df[\"Title\"] = standard_clean(df[\"Title\"])\n",
    "df[\"Meta_description\"] = standard_clean(df[\"Meta_description\"])\n",
    "df[\"Summary\"] = standard_clean(df[\"Summary\"])\n",
    "df[\"Source\"] = standard_clean(df[\"Source\"])\n",
    "\n",
    "df[\"Scraped_at\"] = time_clean(df[\"Scraped_at\"])\n",
    "df[\"Inserted_at\"] = time_clean(df[\"Inserted_at\"])\n",
    "df[\"Updated_at\"] = time_clean(df[\"Updated_at\"])\n",
    "\n",
    "\n",
    "df[\"Content\"] = df[\"Content\"].astype(str)\n",
    "df[\"Content\"] = hard_clean(df[\"Content\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating all dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop duplicates and invalid ID's\n",
    "df.drop_duplicates(keep='first', inplace=True)\n",
    "df = df[df[\"Id\"].str.isnumeric()]\n",
    "\n",
    "# Create relationships for all entities having a direct relationship e.g. article to any other entity\n",
    "df_tags, df_article_tags= create_relationship(df, 'Tags', \"tag_id\", \"label\")\n",
    "df_keywords, df_article_keywords = create_relationship(df, 'Keywords', \"keyword_id\", \"label\")\n",
    "df_authors, df_article_authors = create_relationship(df, 'Authors', \"author_id\", \"name\")\n",
    "df_meta_keywords, df_article_meta_keywords = create_relationship(df, 'Meta_keywords', \"meta_keyword_id\", \"label\")\n",
    "\n",
    "# Creating the url dataframe\n",
    "df_url = df[[\"Id\", \"Url\", \"Domain\"]]\n",
    "df_url[\"Url\"] = df_url[\"Url\"].str.replace(\"www.\", \"\")\n",
    "df_url[\"Domain\"] = df_url[\"Domain\"].str.replace(\"www.\", \"\")\n",
    "df_url[\"Protocol\"] = df_url[\"Url\"].str.split(\"://\", 1, expand=True)[0]\n",
    "df_url[[\"sld\", \"tld\"]] = df_url[\"Domain\"].str.split(\".\", 1, expand=True)\n",
    "df_url[\"slug\"] = df_url[\"Url\"].str.split(\"/\", 3, expand=True)[3]\n",
    "df_url = df_url.assign(url_id=(df_url[\"Url\"]).astype('category').cat.codes)\n",
    "df_url.rename(columns={\"Id\": \"article_id\", \"url_id\": \"id\"}, inplace=True)\n",
    "df_url.drop(columns=\"Domain\", axis=1)\n",
    "df_url = df_url.assign(ssl=(df_url[\"Protocol\"]).astype('category').cat.codes)\n",
    "df_url[\"ssl\"] = df_url[\"ssl\"].astype(str)\n",
    "df_url[\"ssl\"] = df_url[\"ssl\"].str.replace(\"0\", \"No\")\n",
    "df_url[\"ssl\"] = df_url[\"ssl\"].str.replace(\"1\", \"Yes\")\n",
    "df_url = df_url[[\"id\", \"article_id\", \"sld\", \"tld\", \"slug\", \"ssl\"]]\n",
    "df_url[\"slug\"] = standard_clean(df_url[\"slug\"])\n",
    "df_url[\"slug\"] = hard_clean(df_url[\"slug\"])\n",
    "\n",
    "\n",
    "df_authors[\"name\"] = df_authors[\"name\"].replace(np.nan, \"\")\n",
    "df_authors[\"name\"] = df_authors[\"name\"].str.replace(\"nan\", \"\")\n",
    "df_keywords[\"label\"] = df_keywords[\"label\"].replace(np.nan, \"\")\n",
    "df_keywords[\"label\"] = df_keywords[\"label\"].str.replace(\"nan\", \"\")\n",
    "df_meta_keywords[\"label\"] = df_meta_keywords[\"label\"].replace(np.nan, \"\")\n",
    "df_meta_keywords[\"label\"] = df_meta_keywords[\"label\"].str.replace(\"nan\", \"\")\n",
    "df_tags[\"label\"] = df_tags[\"label\"].replace(np.nan, \"\")\n",
    "df_tags[\"label\"] = df_tags[\"label\"].str.replace(\"nan\", \"\")\n",
    "\n",
    "# Creating the article dataframe\n",
    "df_articles = df[[\"Id\", \"Type\", \"Content\", \"Scraped_at\", \"Inserted_at\", \"Updated_at\", \"Title\", \"Meta_description\", \"Summary\", \"Source\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Segmenting the data and writing it into several CSV files. The run time here is more or less equally to the reading of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing entities to csv\n",
    "write_to_csv(df_articles, \"article\")\n",
    "write_to_csv(df_url, \"url\")\n",
    "write_to_csv(df_authors, \"author\")\n",
    "write_to_csv(df_keywords, \"keyword\")\n",
    "write_to_csv(df_meta_keywords, \"meta_keyword\")\n",
    "write_to_csv(df_tags, \"tag\")\n",
    "\n",
    "# Writing relationships to csv\n",
    "write_to_csv(df_article_authors, \"article_author\")\n",
    "write_to_csv(df_article_keywords, \"article_keyword\")\n",
    "write_to_csv(df_article_meta_keywords, \"article_meta_keyword\")\n",
    "write_to_csv(df_article_tags, \"article_tag\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block of code we are copying the data from the CSV files into the SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in the database:  [(998937,)]\n"
     ]
    }
   ],
   "source": [
    "# Connect to an existing database\n",
    "conn = None\n",
    "try:\n",
    "    conn = psycopg2.connect(#database=\"public\",\n",
    "                        port = \"5432\",\n",
    "                        host=\"localhost\", \n",
    "                        user=\"postgres\", \n",
    "                        password=\"1234\")\n",
    "except psycopg2.DatabaseError as e: \n",
    "        print (f'Error {e}')\n",
    "        sys.exit(1)\n",
    "\n",
    "# Open a cursor to perform database operations\n",
    "cur = conn.cursor()\n",
    "cur.execute(drop_tables)\n",
    "cur.execute(create_fake_news_corpus_tables)\n",
    "\n",
    "# Execute a command: this creates a new table\n",
    "\n",
    "with open('article.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"article\", columns=('id','type','content','scraped_at','inserted_at','updated_at','title','meta_description','summary', 'source'), sep=\",\")\n",
    "\n",
    "with open('url.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"url\", columns=('id', \"article_id\", \"sld\", \"tld\", \"slug\", \"ssl\"), sep=\",\")\n",
    "\n",
    "with open('author.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"author\", columns=('id', \"name\"), sep=\",\")\n",
    "\n",
    "with open('keyword.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"keyword\", columns=('id', \"label\"), sep=\",\")\n",
    "\n",
    "with open('meta_keyword.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"meta_keyword\", columns=('id', \"label\"), sep=\",\")\n",
    "\n",
    "with open('tag.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"tag\", columns=('id', \"label\"), sep=\",\")\n",
    "\n",
    "with open('article_author.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"article_author\", columns=('article_id', \"author_id\"), sep=\",\")\n",
    "\n",
    "with open('article_keyword.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"article_keyword\", columns=('article_id', \"keyword_id\"), sep=\",\")\n",
    "\n",
    "with open('article_meta_keyword.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"article_meta_keyword\", columns=('article_id', \"meta_keyword_id\"), sep=\",\")\n",
    "\n",
    "with open('article_tag.csv', \"r\", encoding ='utf-8') as f:\n",
    "        next(f)\n",
    "        cur.copy_from(f, \"article_tag\", columns=('article_id', \"tag_id\"), sep=\",\")\n",
    "\n",
    "#with open('wiki.csv', \"r\", encoding ='utf-8') as f:\n",
    "#        next(f)\n",
    "#        cur.copy_from(f, \"wiki\", columns=('title', 'date', 'content'), sep=\",\")\n",
    "\n",
    "cur.execute(alter_tables)\n",
    "\n",
    "# Proff that the database has the required number of rows\n",
    "cur.execute(\"\"\"SELECT COUNT(*) FROM article\"\"\")\n",
    "f = cur.fetchall()\n",
    "print (\"Number of rows in the database: \" , f)\n",
    "# Make the changes to the database persistent\n",
    "conn.commit()\n",
    "\n",
    "# Close communication with the database\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this block of code we are copying the data from the CSV files into the SQL database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1272.5962235927582\n",
      "Tid i min: 21.2099370598793\n"
     ]
    }
   ],
   "source": [
    "slutTid = time.time()\n",
    "kage2 = slutTid-startTid\n",
    "print(kage2)\n",
    "print(\"Tid i min:\" ,kage2/60)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "425c1cb9f96b02dd326489ac6b96e3042bda6ec4abf5f4b3f4fc167f5a6a6fe0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
